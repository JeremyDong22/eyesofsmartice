# Database Sync Documentation

**Version:** 1.0.0
**Last Updated:** 2025-12-13
**Purpose:** æœ¬åœ°SQLiteæ•°æ®åº“æ‰¹é‡å†™å…¥ä¸Supabaseäº‘ç«¯åŒæ­¥ç­–ç•¥

---

## Overview | ç³»ç»Ÿæ¦‚è¿°

æœ¬ç›®å½•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒè„šæœ¬ï¼Œè§£å†³å®æ—¶è§†é¢‘å¤„ç†ä¸­çš„ä¸¤å¤§æ€§èƒ½ç“¶é¢ˆï¼š

1. **`batch_db_writer.py`** - æ‰¹é‡å†™å…¥ä¼˜åŒ–ï¼ˆ100Ã—æ€§èƒ½æå‡ï¼‰
2. **`sync_to_supabase.py`** - å¢é‡äº‘ç«¯åŒæ­¥ï¼ˆæ•°æ®åº“è®°å½•onlyï¼Œä¸å«åª’ä½“æ–‡ä»¶ï¼‰

### Architecture Philosophy | æ¶æ„å“²å­¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Edge Processing (RTX 3060 Local Machine)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  [Video Processing] â”€â”€â–º [Batch Writer] â”€â”€â–º [SQLite Buffer]     â”‚
â”‚        5 FPS              100 records        24h retention       â”‚
â”‚                          per commit                              â”‚
â”‚                                                                  â”‚
â”‚                            â–¼                                     â”‚
â”‚                                                                  â”‚
â”‚                    [Hourly Sync Job]                            â”‚
â”‚                    (Cron triggered)                             â”‚
â”‚                            â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                   Upload new records only
                   (last 2 hours window)
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cloud Analytics (Supabase PostgreSQL)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ASE_division_states  â†’  Permanent business analytics           â”‚
â”‚  ASE_table_states     â†’  Multi-location dashboards              â”‚
â”‚  ASE_sessions         â†’  Historical trend analysis              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è®¾è®¡åŸåˆ™ (Design Principles):**
- **Local = Fast Transactional Buffer** (SQLite, 24h retention)
- **Cloud = Permanent Analytics Storage** (Supabase, 90 days)
- **No Media Sync** (Database records only, åª’ä½“æ–‡ä»¶ä¸ä¸Šä¼ )
- **Network Fault Tolerance** (æ–­ç½‘è‡ªåŠ¨é‡è¯•ï¼Œä¸‹æ¬¡åŒæ­¥è¡¥ä¸Š)

---

## Script 1: batch_db_writer.py

### Purpose | ç”¨é€”

**Problem (é—®é¢˜):**
è§†é¢‘å¤„ç†è¿‡ç¨‹ä¸­æ¯æ£€æµ‹åˆ°ä¸€æ¬¡çŠ¶æ€å˜åŒ–å°±ç«‹å³commitåˆ°æ•°æ®åº“ï¼Œå¯¼è‡´ï¼š
- 45,000æ¬¡çŠ¶æ€å˜åŒ– = 45,000æ¬¡commit = 37åˆ†é’Ÿå†™å…¥æ—¶é—´
- SQLiteäº‹åŠ¡å¼€é”€æˆä¸ºæ€§èƒ½ç“¶é¢ˆ
- å½±å“å®æ—¶å¤„ç†é€Ÿåº¦

**Solution (è§£å†³æ–¹æ¡ˆ):**
å†…å­˜ç¼“å†²åŒºæ‰¹é‡æäº¤ï¼Œ100æ¡è®°å½•ä¸€æ¬¡commitï¼š
- 45,000æ¬¡çŠ¶æ€å˜åŒ– = 450æ¬¡æ‰¹é‡commit = 22ç§’å†™å…¥æ—¶é—´
- **100Ã— performance improvement (æ€§èƒ½æå‡100å€)**

### Algorithm | ç®—æ³•åŸç†

#### Batch Write Strategy | æ‰¹é‡å†™å…¥ç­–ç•¥

```python
# åŸå§‹æ–¹å¼ (Naive Approach)
for state_change in detections:
    cursor.execute("INSERT INTO division_states ...")
    conn.commit()  # âŒ æ¯æ¡è®°å½•commitä¸€æ¬¡ = æ…¢

# æ‰¹é‡ä¼˜åŒ– (Batch Optimization)
buffer = []
for state_change in detections:
    buffer.append(state_change_tuple)

    if len(buffer) >= BATCH_SIZE:  # Default: 100
        cursor.executemany("INSERT ...", buffer)
        conn.commit()  # âœ… 100æ¡è®°å½•commitä¸€æ¬¡ = å¿«
        buffer.clear()

# æœ€åflushå‰©ä½™è®°å½•
if buffer:
    cursor.executemany("INSERT ...", buffer)
    conn.commit()
```

#### Why It's 100Ã— Faster | ä¸ºä»€ä¹ˆå¿«100å€

**SQLite Transaction Overhead (äº‹åŠ¡å¼€é”€):**

| Operation | Per-Record Commit | Batch Commit (100) |
|-----------|------------------|-------------------|
| **BEGIN transaction** | 45,000æ¬¡ | 450æ¬¡ |
| **Disk sync (fsync)** | 45,000æ¬¡ | 450æ¬¡ |
| **Journal operations** | 45,000æ¬¡ | 450æ¬¡ |
| **COMMIT transaction** | 45,000æ¬¡ | 450æ¬¡ |
| **Total time** | 37 minutes | 22 seconds |

**å…³é”®ä¼˜åŒ–ç‚¹:**
1. **Disk I/O reduction** - fsyncæ˜¯æ…¢é€Ÿæ“ä½œï¼Œå‡å°‘100å€è°ƒç”¨
2. **Journal consolidation** - ä¸€æ¬¡äº‹åŠ¡å†™å…¥å¤šæ¡è®°å½•
3. **Lock contention** - å‡å°‘æ•°æ®åº“é”ç«äº‰

### Usage | ä½¿ç”¨æ–¹æ³•

```python
from batch_db_writer import BatchDatabaseWriter

# Initialize batch writer
db_writer = BatchDatabaseWriter(conn, batch_size=100)

# During video processing (åœ¨å¤„ç†å¾ªç¯ä¸­)
for frame in video_frames:
    detections = detect_state_changes(frame)

    for div_state in division_state_changes:
        db_writer.add_division_state(
            session_id=session_id,
            camera_id=camera_id,
            location_id=location_id,
            frame_number=frame_number,
            timestamp_video=timestamp,
            timestamp_recorded=datetime.now(),
            state="GREEN",  # RED / YELLOW / GREEN
            walking_waiters=2,
            service_waiters=1,
            screenshot_path="/path/to/screenshot.jpg"
        )

    for table_state in table_state_changes:
        db_writer.add_table_state(
            session_id=session_id,
            camera_id=camera_id,
            location_id=location_id,
            frame_number=frame_number,
            timestamp_video=timestamp,
            timestamp_recorded=datetime.now(),
            table_id="T1",
            state="BUSY",  # IDLE / BUSY / CLEANING
            customers_count=4,
            waiters_count=0,
            screenshot_path="/path/to/screenshot.jpg"
        )

# At end of processing (å¤„ç†ç»“æŸæ—¶)
db_writer.flush_all()  # âš ï¸ Critical: æäº¤å‰©ä½™bufferä¸­çš„è®°å½•
stats = db_writer.get_stats()
print(f"Total commits: {stats['total_commits']}")
```

### Configuration | é…ç½®å‚æ•°

```python
# Default configuration
BATCH_SIZE = 100  # Records per commit (æ¯æ‰¹æäº¤çš„è®°å½•æ•°)

# Tuning recommendations (è°ƒä¼˜å»ºè®®):
# - Small batch (10-50): Lower memory, more commits
# - Medium batch (100-200): Balanced (recommended)
# - Large batch (500-1000): Higher memory, fewer commits
#   âš ï¸ Large batches risk data loss if crash before flush
```

### Statistics Tracking | ç»Ÿè®¡ä¿¡æ¯

```python
stats = db_writer.get_stats()
# Returns:
{
    'total_division_inserts': 25000,   # æ€»divisionè®°å½•æ•°
    'total_table_inserts': 20000,      # æ€»tableè®°å½•æ•°
    'total_commits': 450,               # æ€»æäº¤æ¬¡æ•°
    'pending_division': 23,             # Bufferä¸­å¾…æäº¤division
    'pending_table': 15,                # Bufferä¸­å¾…æäº¤table
    'avg_batch_size': 100.0             # å¹³å‡æ¯æ‰¹è®°å½•æ•°
}

db_writer.print_stats()  # Pretty print
```

### Critical Warning | å…³é”®è­¦å‘Š

âš ï¸ **MUST call `flush_all()` at end of processing**
å¦‚æœå¤„ç†è¿‡ç¨‹ä¸­æ–­ï¼ˆcrash/killï¼‰ä¸”æœªè°ƒç”¨`flush_all()`ï¼Œbufferä¸­çš„è®°å½•ä¼šä¸¢å¤±ï¼

**Best Practice:**
```python
try:
    # Video processing loop
    for frame in frames:
        db_writer.add_division_state(...)
finally:
    db_writer.flush_all()  # âœ… ç¡®ä¿æ‰€æœ‰è®°å½•å†™å…¥
```

---

## Script 2: sync_to_supabase.py

### Purpose | ç”¨é€”

**Problem (é—®é¢˜):**
æœ¬åœ°SQLiteæ•°æ®åº“éœ€è¦å®šæœŸåŒæ­¥åˆ°Supabaseäº‘ç«¯ï¼Œä½†ï¼š
- ä¸èƒ½æ¯æ¡è®°å½•éƒ½ç«‹å³ä¸Šä¼ ï¼ˆç½‘ç»œå¼€é”€å¤§ï¼‰
- ä¸èƒ½å…¨é‡ä¸Šä¼ ï¼ˆé‡å¤æ•°æ®ï¼Œæµªè´¹æµé‡ï¼‰
- éœ€è¦å¤„ç†ç½‘ç»œæ•…éšœå’Œé‡è¯•

**Solution (è§£å†³æ–¹æ¡ˆ):**
å¢é‡åŒæ­¥ç­–ç•¥ + æ ‡è®°æœºåˆ¶ï¼š
- **Hourly sync** - æ¯å°æ—¶ä¸Šä¼ æ–°è®°å½•ï¼ˆlast 2 hours windowï¼‰
- **Incremental upload** - åªä¸Šä¼ `synced_to_cloud=0`çš„è®°å½•
- **Local cleanup** - ä¸Šä¼ æˆåŠŸååˆ é™¤24å°æ—¶å‰çš„æœ¬åœ°è®°å½•
- **Retry queue** - ç½‘ç»œæ•…éšœæ—¶ä¸‹æ¬¡è‡ªåŠ¨é‡è¯•

### Sync Strategy | åŒæ­¥ç­–ç•¥

#### Two Sync Modes | ä¸¤ç§åŒæ­¥æ¨¡å¼

**1. Hourly Sync (æ¯å°æ—¶å¢é‡åŒæ­¥)**

```bash
# Cron job (æ¯å°æ—¶è§¦å‘)
0 * * * * python3 sync_to_supabase.py --mode hourly
```

```
Time Window: Last 2 hours (2å°æ—¶çª—å£ï¼Œæœ‰é‡å ä¿é™©)
Records: WHERE synced_to_cloud = 0 AND created_at >= NOW() - 2h
Purpose: æ­£å¸¸è¿è¥æœŸé—´çš„å®æ—¶åŒæ­¥
```

**2. Full Sync (å…¨é‡åŒæ­¥ - æ•…éšœæ¢å¤)**

```bash
# Manual or after network outage (æ‰‹åŠ¨æˆ–ç½‘ç»œæ¢å¤å)
python3 sync_to_supabase.py --mode full
```

```
Time Window: All unsynced (æ‰€æœ‰æœªåŒæ­¥è®°å½•)
Records: WHERE synced_to_cloud = 0
Purpose: ç½‘ç»œæ•…éšœåè¡¥å…¨ç¼ºå¤±æ•°æ®
```

#### Incremental Sync Algorithm | å¢é‡åŒæ­¥ç®—æ³•

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Query Unsynced Records (æŸ¥è¯¢æœªåŒæ­¥è®°å½•)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SELECT * FROM division_states                              â”‚
â”‚  WHERE synced_to_cloud = 0                                  â”‚
â”‚    AND created_at >= NOW() - 2 hours  (hourly mode only)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Upload in Batches (æ‰¹é‡ä¸Šä¼ )                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BATCH_SIZE = 1000 records per batch                        â”‚
â”‚                                                              â”‚
â”‚  for batch in chunks(records, 1000):                        â”‚
â”‚      transformed = [transform(r) for r in batch]            â”‚
â”‚      supabase.table('ASE_division_states').insert(...)      â”‚
â”‚      mark_as_synced(batch)  # Update local DB               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Mark as Synced (æ ‡è®°å·²åŒæ­¥)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  UPDATE division_states                                      â”‚
â”‚  SET synced_to_cloud = 1                                    â”‚
â”‚  WHERE division_state_id IN (uploaded_ids)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Cleanup Old Synced Data (æ¸…ç†æ—§æ•°æ®)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DELETE FROM division_states                                â”‚
â”‚  WHERE synced_to_cloud = 1                                  â”‚
â”‚    AND created_at < NOW() - 24 hours                        â”‚
â”‚                                                              â”‚
â”‚  Keeps 24h local buffer for safety (ä¿ç•™24å°æ—¶æœ¬åœ°ç¼“å†²)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Transformation | æ•°æ®è½¬æ¢

#### SQLite â†’ Supabase Schema Mapping

**Division States:**

```python
# SQLite Row (æœ¬åœ°)
{
    'division_state_id': 12345,           # æœ¬åœ°ä¸»é”® (ä¸ä¸Šä¼ )
    'session_id': '20251213_143000_camera_35',
    'camera_id': 'camera_35',
    'location_id': 'mianyang_yebailinghotpot_1958commercialdistrict',
    'frame_number': 1500,
    'timestamp_video': 300.5,
    'timestamp_recorded': '2025-12-13 14:35:00',
    'state': 'GREEN',
    'walking_area_waiters': 2,
    'service_area_waiters': 1,
    'total_staff': 3,
    'screenshot_path': '/path/to/local/screenshot.jpg',  # æœ¬åœ°è·¯å¾„ (ä¸ä¸Šä¼ )
    'synced_to_cloud': 0,                 # åŒæ­¥æ ‡è®° (ä¸ä¸Šä¼ )
    'created_at': '2025-12-13 14:35:01'   # æœ¬åœ°åˆ›å»ºæ—¶é—´ (ä¸ä¸Šä¼ )
}

# Transformed for Supabase (äº‘ç«¯)
{
    # division_state_id ç”±Supabaseè‡ªåŠ¨ç”Ÿæˆ (SERIAL)
    'session_id': '20251213_143000_camera_35',
    'camera_id': 'camera_35',
    'location_id': 'mianyang_yebailinghotpot_1958commercialdistrict',
    'frame_number': 1500,
    'timestamp_video': 300.5,
    'timestamp_recorded': '2025-12-13 14:35:00',
    'state': 'GREEN',
    'walking_area_waiters': 2,
    'service_area_waiters': 1,
    'total_staff': 3
    # screenshot_path ä¸åŒ…å«åœ¨äº‘ç«¯ (åª’ä½“æ–‡ä»¶ä¸ä¸Šä¼ )
    # created_at ç”±Supabaseè‡ªåŠ¨å¡«å……
}
```

**Table States:**

```python
# SQLite â†’ Supabase (similar structure)
{
    'session_id': '20251213_143000_camera_35',
    'camera_id': 'camera_35',
    'location_id': 'mianyang_yebailinghotpot_1958commercialdistrict',
    'frame_number': 1500,
    'timestamp_video': 300.5,
    'timestamp_recorded': '2025-12-13 14:35:00',
    'table_id': 'T1',
    'state': 'BUSY',
    'customers_count': 4,
    'waiters_count': 0
    # screenshot_path omitted (ä¸ä¸Šä¼ æˆªå›¾è·¯å¾„)
}
```

**Key Points:**
- âœ… **Primary keys** - Supabaseç”¨SERIALè‡ªåŠ¨ç”Ÿæˆï¼Œä¸ä¼ local ID
- âŒ **Screenshots** - è·¯å¾„ä¸ä¸Šä¼ ï¼Œåª’ä½“æ–‡ä»¶ä¿ç•™æœ¬åœ°
- âŒ **Sync flags** - `synced_to_cloud`ä»…æœ¬åœ°ä½¿ç”¨
- âœ… **Timestamps** - ä¸šåŠ¡æ—¶é—´æˆ³æ­£å¸¸ä¸Šä¼ 

### Conflict Resolution | å†²çªè§£å†³

#### Duplicate Prevention | é˜²æ­¢é‡å¤ä¸Šä¼ 

**Scenario (åœºæ™¯):**
æ‰‹åŠ¨full sync + å®šæ—¶hourly syncåŒæ—¶è¿è¡Œï¼Œå¯èƒ½é‡å¤ä¸Šä¼ 

**Solution (è§£å†³æ–¹æ¡ˆ):**

```python
# 1. Local flag prevents duplicate queries (æœ¬åœ°æ ‡è®°é˜²æ­¢é‡æŸ¥)
WHERE synced_to_cloud = 0  # å·²ä¸Šä¼ çš„è®°å½•ä¸ä¼šå†æŸ¥è¯¢

# 2. Supabase unique constraints prevent duplicate inserts
# Supabase schemaä¸­çš„UNIQUEçº¦æŸé˜²æ­¢é‡å¤æ’å…¥:
UNIQUE(session_id, camera_id, frame_number, table_id)  # Table states
UNIQUE(session_id, camera_id, frame_number)            # Division states

# 3. Error handling: Ignore duplicate key errors
try:
    supabase.table('ASE_division_states').insert(batch)
except Exception as e:
    if "duplicate key" in str(e):
        # Already uploaded, mark as synced (å·²å­˜åœ¨ï¼Œç›´æ¥æ ‡è®°)
        mark_as_synced(batch)
    else:
        # Real error, log and retry later (çœŸå®é”™è¯¯ï¼Œè®°å½•å¹¶é‡è¯•)
        log_error(e)
```

#### Network Failure Handling | ç½‘ç»œæ•…éšœå¤„ç†

**Tolerance Strategy (å®¹é”™ç­–ç•¥):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Batch Upload with Per-Batch Error Handling                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  for i in range(0, len(records), BATCH_SIZE):              â”‚
â”‚      batch = records[i:i+BATCH_SIZE]                       â”‚
â”‚                                                             â”‚
â”‚      try:                                                   â”‚
â”‚          supabase.table(...).insert(batch)                 â”‚
â”‚          mark_as_synced(batch)  # âœ… æˆåŠŸæ ‡è®°               â”‚
â”‚                                                             â”‚
â”‚      except NetworkError:                                  â”‚
â”‚          log_error(f"Batch {i} failed")                    â”‚
â”‚          continue  # âš ï¸ ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹ï¼Œä¸å…¨éƒ¨å¤±è´¥          â”‚
â”‚                                                             â”‚
â”‚  Result:                                                    â”‚
â”‚  - Partial success: éƒ¨åˆ†æ‰¹æ¬¡æˆåŠŸä¸Šä¼                         â”‚
â”‚  - Failed batches: ä¿æŒsynced_to_cloud=0                   â”‚
â”‚  - Next sync: è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è®°å½•                            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Not Fail Fast? (ä¸ºä»€ä¹ˆä¸ç«‹å³å¤±è´¥)**
- ç½‘ç»œæŠ–åŠ¨å¯èƒ½åªå½±å“éƒ¨åˆ†è¯·æ±‚
- æˆåŠŸä¸Šä¼ çš„æ‰¹æ¬¡ä¸éœ€è¦é‡ä¼ ï¼ˆèŠ‚çœæµé‡ï¼‰
- å¤±è´¥æ‰¹æ¬¡ä¸‹æ¬¡è‡ªåŠ¨é‡è¯•ï¼ˆæœ€ç»ˆä¸€è‡´æ€§ï¼‰

### Usage | ä½¿ç”¨æ–¹æ³•

#### Command Line

```bash
# Hourly sync (2-hour window, recommended for cron)
python3 sync_to_supabase.py --mode hourly

# Full sync (all unsynced records, after network outage)
python3 sync_to_supabase.py --mode full

# Dry run (test without actual upload)
python3 sync_to_supabase.py --mode hourly --dry-run
```

#### Environment Variables | ç¯å¢ƒå˜é‡

```bash
# Required (å¿…é¡»é…ç½®)
export SUPABASE_URL="https://wdpeoyugsxqnpwwtkqsl.supabase.co"
export SUPABASE_ANON_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# Add to ~/.bashrc or /etc/environment
```

#### Cron Job Setup | å®šæ—¶ä»»åŠ¡é…ç½®

```bash
# Hourly sync during operating + processing hours
# æ¯å°æ—¶åŒæ­¥ (11:00 AM - 11:00 PM)
0 11-23 * * * cd /path/to/production/RTX_3060/scripts/database_sync && python3 sync_to_supabase.py --mode hourly >> /var/log/ase_sync.log 2>&1

# Full sync at 3 AM (catch any missed records)
# å‡Œæ™¨3ç‚¹å…¨é‡åŒæ­¥ï¼ˆæ•è·é—æ¼è®°å½•ï¼‰
0 3 * * * cd /path/to/production/RTX_3060/scripts/database_sync && python3 sync_to_supabase.py --mode full >> /var/log/ase_sync.log 2>&1
```

### Configuration | é…ç½®å‚æ•°

```python
# Database location
DB_PATH = PROJECT_ROOT / "db" / "detection_data.db"

# Upload batch size (ä¸Šä¼ æ‰¹æ¬¡å¤§å°)
BATCH_SIZE = 1000  # Records per HTTP request

# Local retention (æœ¬åœ°ä¿ç•™æœŸ)
RETENTION_HOURS = 24  # Keep synced data for 24h before cleanup

# Time windows (æ—¶é—´çª—å£)
HOURLY_WINDOW = 2  # Hours (2å°æ—¶é‡å çª—å£)
```

### Monitoring | ç›‘æ§

#### Sync Status Table | åŒæ­¥çŠ¶æ€è¡¨

```sql
-- Check recent sync operations (æŸ¥çœ‹æœ€è¿‘åŒæ­¥è®°å½•)
SELECT sync_type, last_sync_time, records_synced, status, error_message
FROM sync_status
ORDER BY last_sync_time DESC
LIMIT 10;

-- Example output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sync_type        â”‚ last_sync_time      â”‚ records_synced  â”‚ status  â”‚ error_message â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hourly           â”‚ 2025-12-13 15:00:00 â”‚ 2345           â”‚ success â”‚ NULL          â”‚
â”‚ hourly           â”‚ 2025-12-13 14:00:00 â”‚ 1987           â”‚ success â”‚ NULL          â”‚
â”‚ hourly           â”‚ 2025-12-13 13:00:00 â”‚ 2103           â”‚ partial â”‚ 2 batch errorsâ”‚
â”‚ full             â”‚ 2025-12-13 03:00:00 â”‚ 45678          â”‚ success â”‚ NULL          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Check Unsynced Records | æ£€æŸ¥æœªåŒæ­¥è®°å½•

```sql
-- Count pending records (å¾…ä¸Šä¼ è®°å½•æ•°)
SELECT 'division_states' as table_name, COUNT(*) as pending_count
FROM division_states
WHERE synced_to_cloud = 0

UNION ALL

SELECT 'table_states', COUNT(*)
FROM table_states
WHERE synced_to_cloud = 0;

-- Example output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ table_name       â”‚ pending_count â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ division_states  â”‚ 234           â”‚  # âš ï¸ If large, check network/logs
â”‚ table_states     â”‚ 567           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Log Files | æ—¥å¿—æ–‡ä»¶

```bash
# View sync logs
tail -f /var/log/ase_sync.log

# Example log output:
# 2025-12-13 15:00:01 - â° Hourly Sync Mode
# 2025-12-13 15:00:01 - ğŸ“ Location: mianyang_yebailinghotpot_1958commercialdistrict
# 2025-12-13 15:00:02 - ğŸ“¹ Syncing video metadata...
# 2025-12-13 15:00:02 -    No videos to sync
# 2025-12-13 15:00:03 - ğŸ¬ Syncing sessions...
# 2025-12-13 15:00:03 -    âœ… Synced 3 sessions
# 2025-12-13 15:00:05 - ğŸ”´ğŸŸ¡ğŸŸ¢ Syncing division states...
# 2025-12-13 15:00:05 -    Uploaded 1000/2345...
# 2025-12-13 15:00:07 -    Uploaded 2000/2345...
# 2025-12-13 15:00:08 -    âœ… Synced 2345 division state changes
# 2025-12-13 15:00:10 - ğŸ“Š Syncing table states...
# 2025-12-13 15:00:12 -    âœ… Synced 1987 table state changes
# 2025-12-13 15:00:13 - ğŸ—‘ï¸  Cleaning up synced data older than 24h...
# 2025-12-13 15:00:13 -    Deleted 3456 division states
# 2025-12-13 15:00:13 -    Deleted 2789 table states
# 2025-12-13 15:00:14 - âœ… Sync completed successfully!
```

---

## Data Flow Summary | æ•°æ®æµæ€»ç»“

### Complete Pipeline | å®Œæ•´æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Video Processing (å®æ—¶å¤„ç†)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Process video at 5 FPS                                        â”‚
â”‚  Detect state changes (debounce: 1s)                           â”‚
â”‚  â”œâ”€â–º Division state change â†’ BatchWriter.add_division_state() â”‚
â”‚  â””â”€â–º Table state change â†’ BatchWriter.add_table_state()       â”‚
â”‚                                                                 â”‚
â”‚  Performance: 3.24Ã— real-time processing                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼ Auto-flush every 100 records
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Batch Database Write (æ‰¹é‡å†™å…¥)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Buffer: In-memory queue (100 records)                         â”‚
â”‚  Commit: executemany() + conn.commit()                         â”‚
â”‚  Speed: 100Ã— faster than per-record commits                    â”‚
â”‚                                                                 â”‚
â”‚  Result: division_states (synced_to_cloud = 0)                 â”‚
â”‚          table_states (synced_to_cloud = 0)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼ Hourly (cron)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Incremental Cloud Sync (å¢é‡äº‘ç«¯åŒæ­¥)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query: WHERE synced_to_cloud = 0 AND created_at >= NOW()-2h  â”‚
â”‚  Upload: 1000 records per batch to Supabase                    â”‚
â”‚  Mark: SET synced_to_cloud = 1                                 â”‚
â”‚                                                                 â”‚
â”‚  Network failure: Continue with next batch (partial success)   â”‚
â”‚  Duplicate detection: Unique constraints prevent re-upload     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼ After successful sync
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Local Cleanup (æœ¬åœ°æ¸…ç†)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DELETE FROM division_states                                   â”‚
â”‚  WHERE synced_to_cloud = 1 AND created_at < NOW() - 24h       â”‚
â”‚                                                                 â”‚
â”‚  Result: SQLite database keeps 24h buffer only                 â”‚
â”‚          Supabase has permanent copy (90 days retention)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Storage Lifecycle | å­˜å‚¨ç”Ÿå‘½å‘¨æœŸ

| Time | Local SQLite | Supabase Cloud | Description |
|------|--------------|----------------|-------------|
| **T+0** | âœ… Written (synced=0) | âŒ Not yet | å®æ—¶å¤„ç†å†™å…¥æœ¬åœ° |
| **T+1h** | âœ… Exists (synced=1) | âœ… Uploaded | æ¯å°æ—¶åŒæ­¥åˆ°äº‘ç«¯ |
| **T+24h** | âŒ Deleted | âœ… Exists | æœ¬åœ°æ¸…ç†ï¼Œäº‘ç«¯ä¿ç•™ |
| **T+90d** | âŒ Deleted | âš ï¸ Archive? | äº‘ç«¯å¯èƒ½å½’æ¡£æˆ–æ¸…ç† |

**Key Insight (å…³é”®ç†è§£):**
æœ¬åœ°SQLiteæ˜¯**24å°æ—¶æ»šåŠ¨ç¼“å†²åŒº**ï¼Œäº‘ç«¯Supabaseæ˜¯**æ°¸ä¹…ä¸šåŠ¡æ•°æ®åº“**

---

## Performance Characteristics | æ€§èƒ½ç‰¹å¾

### Batch Writer Performance | æ‰¹é‡å†™å…¥æ€§èƒ½

| Metric | Per-Record Commit | Batch Commit (100) | Improvement |
|--------|------------------|-------------------|-------------|
| **Total inserts** | 45,000 | 45,000 | - |
| **Total commits** | 45,000 | 450 | **100Ã—** |
| **Disk syncs (fsync)** | 45,000 | 450 | **100Ã—** |
| **Write time** | 37 minutes | 22 seconds | **100Ã—** |
| **Throughput** | 20 records/sec | 2,045 records/sec | **100Ã—** |

**Bottleneck Analysis:**
- âŒ **Before:** SQLite transaction overhead (fsync every record)
- âœ… **After:** Minimal transaction overhead (fsync every 100 records)

### Sync Performance | åŒæ­¥æ€§èƒ½

**Typical Hourly Sync (1 camera, 1 hour footage):**

```
Video processing: 1 hour footage â†’ ~3,000 state changes
Batch upload: 3,000 records Ã· 1000 per batch = 3 HTTP requests
Network time: ~3-5 seconds total (stable network)
Database cleanup: ~1 second
Total sync time: <10 seconds
```

**Full Sync After 8-Hour Network Outage:**

```
Unsynced records: 8 hours Ã— 3,000/hour = 24,000 records
Batch upload: 24,000 Ã· 1000 = 24 HTTP requests
Network time: ~30-40 seconds
Database cleanup: ~2 seconds
Total sync time: <60 seconds
```

**Network Bandwidth:**

```
Average record size: ~200 bytes (JSON payload)
Hourly upload: 3,000 records Ã— 200 bytes = 600 KB
Daily upload: 7.5 hours Ã— 600 KB = 4.5 MB/day/camera
Monthly upload: 4.5 MB Ã— 30 days = 135 MB/month/camera

Conclusion: ç½‘ç»œæµé‡æå°ï¼Œå®Œå…¨å¯æ¥å—
```

---

## Troubleshooting | æ•…éšœæ’æŸ¥

### Issue 1: Batch Writer Not Flushing | æ‰¹é‡å†™å…¥æœªæäº¤

**Symptoms (ç—‡çŠ¶):**
- æ•°æ®åº“ä¸­è®°å½•æ•°å°‘äºé¢„æœŸ
- å¤„ç†ç»“æŸåbufferä¸­æœ‰pendingè®°å½•

**Diagnosis (è¯Šæ–­):**
```python
stats = db_writer.get_stats()
print(stats)
# {'pending_division': 45, 'pending_table': 23}  # âš ï¸ Should be 0
```

**Solution (è§£å†³):**
```python
# Always call flush_all() at end
try:
    # Processing loop
    for frame in frames:
        db_writer.add_division_state(...)
finally:
    db_writer.flush_all()  # âœ… Critical
```

### Issue 2: Sync Fails with "Network Error" | ç½‘ç»œé”™è¯¯

**Symptoms (ç—‡çŠ¶):**
```bash
# Log shows:
âŒ Upload failed for batch 1: Network timeout
âŒ Upload failed for batch 2: Connection refused
```

**Diagnosis (è¯Šæ–­):**
```bash
# Check network connectivity
ping supabase.co

# Check Supabase endpoint
curl https://wdpeoyugsxqnpwwtkqsl.supabase.co

# Check environment variables
echo $SUPABASE_URL
echo $SUPABASE_ANON_KEY
```

**Solution (è§£å†³):**
```bash
# 1. Network restored â†’ automatic retry on next cron
# 2. Large backlog â†’ manual full sync
python3 sync_to_supabase.py --mode full

# 3. Check unsynced count
sqlite3 ../db/detection_data.db "SELECT COUNT(*) FROM division_states WHERE synced_to_cloud = 0"
```

### Issue 3: Duplicate Key Error | é‡å¤é”®é”™è¯¯

**Symptoms (ç—‡çŠ¶):**
```
âŒ Upload failed: duplicate key value violates unique constraint
```

**Diagnosis (è¯Šæ–­):**
```sql
-- Check if already in Supabase
SELECT * FROM ASE_division_states
WHERE session_id = '20251213_143000_camera_35'
  AND frame_number = 1500;
```

**Solution (è§£å†³):**
```bash
# This is usually safe to ignore (data already uploaded)
# Mark local records as synced:
python3 -c "
import sqlite3
conn = sqlite3.connect('../db/detection_data.db')
conn.execute('UPDATE division_states SET synced_to_cloud = 1 WHERE session_id = ?', ['20251213_143000_camera_35'])
conn.commit()
"
```

### Issue 4: Local Database Growing Too Large | æœ¬åœ°æ•°æ®åº“è†¨èƒ€

**Symptoms (ç—‡çŠ¶):**
```bash
du -h ../db/detection_data.db
# 15G  # âš ï¸ Should be < 1GB
```

**Diagnosis (è¯Šæ–­):**
```sql
-- Check unsynced count
SELECT
    COUNT(*) FILTER (WHERE synced_to_cloud = 0) as unsynced,
    COUNT(*) FILTER (WHERE synced_to_cloud = 1) as synced,
    COUNT(*) as total
FROM division_states;

-- Example problem output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ unsynced â”‚ synced  â”‚ total   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 234567   â”‚ 1234567 â”‚ 1469134 â”‚  # âš ï¸ Too many synced records not cleaned
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Solution (è§£å†³):**
```bash
# 1. Force sync unsynced records
python3 sync_to_supabase.py --mode full

# 2. Force cleanup old synced records
sqlite3 ../db/detection_data.db <<EOF
DELETE FROM division_states WHERE synced_to_cloud = 1 AND created_at < datetime('now', '-24 hours');
DELETE FROM table_states WHERE synced_to_cloud = 1 AND created_at < datetime('now', '-24 hours');
VACUUM;  -- Reclaim disk space
EOF
```

---

## Best Practices | æœ€ä½³å®è·µ

### For Batch Writer | æ‰¹é‡å†™å…¥

1. âœ… **Always flush at end** - Use try/finally to ensure flush
2. âœ… **Monitor buffer size** - Check stats periodically
3. âœ… **Use default batch size** - 100 records is well-tested
4. âŒ **Don't increase batch too much** - Risk data loss on crash
5. âœ… **Print stats on completion** - Helps debugging

### For Cloud Sync | äº‘ç«¯åŒæ­¥

1. âœ… **Use hourly sync** - Cron job for automated operation
2. âœ… **Keep 24h local buffer** - Safety margin for retries
3. âœ… **Monitor sync_status table** - Check for failures
4. âœ… **Run full sync after outages** - Catch up missed records
5. âŒ **Don't sync too frequently** - Hourly is optimal
6. âœ… **Use dry-run for testing** - Validate before production

### For Production | ç”Ÿäº§ç¯å¢ƒ

1. âœ… **Set environment variables** - SUPABASE_URL + KEY
2. âœ… **Enable cron logging** - Redirect to /var/log/ase_sync.log
3. âœ… **Monitor disk space** - Local DB should stay < 1GB
4. âœ… **Check Supabase dashboard** - Verify data arriving
5. âœ… **Implement alerts** - Notify if sync fails repeatedly

---

## Version History | ç‰ˆæœ¬å†å²

**1.0.0** (2025-11-15)
- âœ… Initial implementation of batch_db_writer.py
- âœ… Initial implementation of sync_to_supabase.py
- âœ… 100Ã— performance improvement for database writes
- âœ… Hourly incremental sync to Supabase
- âœ… Network fault tolerance with partial success
- âœ… 24-hour local buffer with automatic cleanup

---

## Related Documentation | ç›¸å…³æ–‡æ¡£

- **Database Schema:** [/db/database_schema.sql](../../db/database_schema.sql)
- **Cloud Architecture:** [/db/CLAUDE.md](../../db/CLAUDE.md)
- **Video Processing:** [/scripts/video_processing/CLAUDE.md](../video_processing/CLAUDE.md)
- **Deployment Guide:** [/scripts/deployment/CLAUDE.md](../deployment/CLAUDE.md)

---

**Maintained By:** ASE Development Team
**Contact:** For sync issues or performance questions, check logs first: `/var/log/ase_sync.log`
